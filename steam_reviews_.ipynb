{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_full = pd.read_csv('data\\\\steam_reviews.csv')\n",
    "\n",
    "df = df_full[:500000]\n",
    "\n",
    "#app_names = df_sample.groupby('app_name')\n",
    "#print(app_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_name\n",
      "18 Wheels of Steel: American Long Haul      165\n",
      "3D Ultra Minigolf Adventures Deluxe          37\n",
      "7 Wonders II                                 26\n",
      "7 Wonders: The Treasures of Seven            20\n",
      "9th Company - Roots of Terror                72\n",
      "                                          ...  \n",
      "Worms Revolution                           1478\n",
      "XCOM: Enemy Unknown                       19654\n",
      "Zero Gear                                    45\n",
      "iBomber Defense                              97\n",
      "inMomentum                                   95\n",
      "Name: count, Length: 319, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of reviews per game\n",
    "app_names = df.groupby('app_name')['app_name'].value_counts()\n",
    "print(app_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "-1     66819\n",
      " 1    433181\n",
      "Name: count, dtype: int64\n",
      "review_score\n",
      "-1    353.943669\n",
      " 1    261.358792\n",
      "Name: review_length, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_6648\\3646611418.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_length'] = df['review_text'].apply(strLen)\n"
     ]
    }
   ],
   "source": [
    "recommended = df.groupby('review_score')['review_score'].value_counts()\n",
    "print(recommended)\n",
    "\n",
    "def strLen(text):\n",
    "    return len(str(text))\n",
    "\n",
    "# k√∂r denna efter texten bearbetats?\n",
    "df['review_length'] = df['review_text'].apply(strLen)\n",
    "\n",
    "mean_len = df.groupby('review_score')['review_length'].mean()\n",
    "print(mean_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data visualization\n",
    "# add wordcloud after stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_6648\\4256761081.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_clean'] = df['review_text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    #text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "#print(df_sample.iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Johan\\Documents\\GitHub\\NLP-projects\\steam_reviews_.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstopwords\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stop_words \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39mget_stopwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#stop_words = stopwords.words('english')\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stopwords'"
     ]
    }
   ],
   "source": [
    "import stopwords\n",
    "stop_words = stopwords.get_stopwords('english')\n",
    "#stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_clean'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Johan\\Documents\\GitHub\\NLP-projects\\steam_reviews_.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m stemmer \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mSnowballStemmer(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstemm_text\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_clean'].apply(stemm_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johan\\AppData\\Local\\Temp\\ipykernel_6648\\1713298668.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_score_target'] = df['review_score'].apply(targetEncoded)\n"
     ]
    }
   ],
   "source": [
    "def targetEncoded(toEncode):\n",
    "    return 1 if toEncode == 1 else 0\n",
    "\n",
    "df['review_score_target'] = df['review_score'].apply(targetEncoded)\n",
    "\n",
    "x = df['review_clean']\n",
    "y = df['review_score_target']\n",
    "\n",
    "print(len(x), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375000 375000\n",
      "125000 125000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(stop_words=None, ngram_range=(1, 1), max_features=100)\n",
    "vect.fit(x_train) # Creates a dictionary of all the words and maps each word in the output matrix\n",
    "\n",
    "x_train_dtm = vect.transform(x_train) # Transforms the dataset to the vect dictonary\n",
    "x_test_dtm = vect.transform(x_test) # Transforms the dataset to the vect dictonary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n",
    "#x_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     abail  abandon  abe  abil  abject       abl  abort  abound  about  \\\n",
      "0      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "1      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "2      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "3      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "4      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "..     ...      ...  ...   ...     ...       ...    ...     ...    ...   \n",
      "995    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "996    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "997    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "998    0.0      0.0  0.0   0.0     0.0  0.237555    0.0     0.0    0.0   \n",
      "999    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "\n",
      "     abrupt  ...  zambi  zelda  zeldaroguelik  zerog  zip  zombi  zone  \\\n",
      "0       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "1       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "2       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "3       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "4       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "..      ...  ...    ...    ...            ...    ...  ...    ...   ...   \n",
      "995     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "996     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "997     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "998     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "999     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "\n",
      "         zoom  zue   ·Öö·Öö  \n",
      "0    0.000000  0.0  0.0  \n",
      "1    0.000000  0.0  0.0  \n",
      "2    0.000000  0.0  0.0  \n",
      "3    0.027886  0.0  0.0  \n",
      "4    0.000000  0.0  0.0  \n",
      "..        ...  ...  ...  \n",
      "995  0.000000  0.0  0.0  \n",
      "996  0.000000  0.0  0.0  \n",
      "997  0.000000  0.0  0.0  \n",
      "998  0.000000  0.0  0.0  \n",
      "999  0.000000  0.0  0.0  \n",
      "\n",
      "[1000 rows x 4467 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get weights of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "tfidf_matrix = vect.fit_transform(x_train[:1000])\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98719733 0.45785863 0.8969804  ... 0.86328383 0.97517055 0.88208671]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.24      0.33     16673\n",
      "           1       0.89      0.97      0.93    108327\n",
      "\n",
      "    accuracy                           0.87    125000\n",
      "   macro avg       0.72      0.60      0.63    125000\n",
      "weighted avg       0.85      0.87      0.85    125000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.796004679317431"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.25 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Johan\\Documents\\GitHub\\NLP-projects\\steam_reviews_.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#shap.initjs()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews_.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_test_dtm_sample \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39msample(x_train_dtm, \u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.42.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_explanation\u001b[39;00m \u001b[39mimport\u001b[39;00m Explanation, Cohorts\n\u001b[0;32m      7\u001b[0m \u001b[39m# explainers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexplainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_explainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Explainer\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\_explanation.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslicer\u001b[39;00m \u001b[39mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DimensionError\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m OpChain\n\u001b[0;32m     16\u001b[0m \u001b[39m# slicer confuses pylint...\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# pylint: disable=no-member\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_clustering\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     delta_minimization_order,\n\u001b[0;32m      3\u001b[0m     hclust,\n\u001b[0;32m      4\u001b[0m     hclust_ordering,\n\u001b[0;32m      5\u001b[0m     partition_tree,\n\u001b[0;32m      6\u001b[0m     partition_tree_shuffle,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     OpChain,\n\u001b[0;32m     10\u001b[0m     approximate_interactions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     suppress_stderr,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_masked_model\u001b[39;00m \u001b[39mimport\u001b[39;00m MaskedModel, make_masks\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\utils\\_clustering.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m njit\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m safe_isinstance\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_show_progress\u001b[39;00m \u001b[39mimport\u001b[39;00m show_progress\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\numba\\__init__.py:55\u001b[0m\n\u001b[0;32m     50\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mscipy\u001b[39m.\u001b[39m__version__\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m---> 55\u001b[0m _ensure_critical_deps()\n\u001b[0;32m     56\u001b[0m \u001b[39m# END DO NOT MOVE\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\numba\\__init__.py:42\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m     41\u001b[0m \u001b[39melif\u001b[39;00m numpy_version \u001b[39m>\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m25\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumba needs NumPy 1.25 or less\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.25 or less"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "#shap.initjs()\n",
    "x_test_dtm_sample = shap.sample(x_train_dtm, 10)\n",
    "explainer = shap.KernelExplainer(nb.predict, x_test_dtm_sample)\n",
    "shap_values = explainer.shap_values(x_test_dtm_sample)\n",
    "shap.summary_plot(shap_values, x_test_dtm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-projects-czQ2DZYv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
