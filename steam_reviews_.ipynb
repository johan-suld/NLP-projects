{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_full = pd.read_csv('data\\\\steam_reviews.csv')\n",
    "\n",
    "df = df_full[:500000]\n",
    "\n",
    "#app_names = df_sample.groupby('app_name')\n",
    "#print(app_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_name\n",
      "18 Wheels of Steel: American Long Haul      165\n",
      "3D Ultra Minigolf Adventures Deluxe          37\n",
      "7 Wonders II                                 26\n",
      "7 Wonders: The Treasures of Seven            20\n",
      "9th Company - Roots of Terror                72\n",
      "                                          ...  \n",
      "Worms Revolution                           1478\n",
      "XCOM: Enemy Unknown                       19654\n",
      "Zero Gear                                    45\n",
      "iBomber Defense                              97\n",
      "inMomentum                                   95\n",
      "Name: count, Length: 319, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get number of reviews per game\n",
    "app_names = df.groupby('app_name')['app_name'].value_counts()\n",
    "print(app_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_score\n",
      "-1     66819\n",
      " 1    433181\n",
      "Name: count, dtype: int64\n",
      "review_score\n",
      "-1    353.943669\n",
      " 1    261.358792\n",
      "Name: review_length, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "recommended = df.groupby('review_score')['review_score'].value_counts()\n",
    "print(recommended)\n",
    "\n",
    "def strLen(text):\n",
    "    return len(str(text))\n",
    "\n",
    "# k√∂r denna efter texten bearbetats?\n",
    "df['review_length'] = df['review_text'].apply(strLen)\n",
    "\n",
    "mean_len = df.groupby('review_score')['review_length'].mean()\n",
    "print(mean_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data visualization\n",
    "# add wordcloud after stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    #text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    #text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "#print(df_sample.iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import stopwords\n",
    "stop_words = stopwords.get_stopwords('english')\n",
    "#stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_clean'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review_clean'].apply(stemm_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def targetEncoded(toEncode):\n",
    "    return 1 if toEncode == 1 else 0\n",
    "\n",
    "df['review_score_target'] = df['review_score'].apply(targetEncoded)\n",
    "\n",
    "x = df['review_clean']\n",
    "y = df['review_score_target']\n",
    "\n",
    "print(len(x), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375000 375000\n",
      "125000 125000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(stop_words=None, ngram_range=(1, 1), max_features=100)\n",
    "vect.fit(x_train) # Creates a dictionary of all the words and maps each word in the output matrix\n",
    "\n",
    "x_train_dtm = vect.transform(x_train) # Transforms the dataset to the vect dictonary\n",
    "x_test_dtm = vect.transform(x_test) # Transforms the dataset to the vect dictonary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n",
    "#x_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     abail  abandon  abe  abil  abject       abl  abort  abound  about  \\\n",
      "0      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "1      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "2      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "3      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "4      0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "..     ...      ...  ...   ...     ...       ...    ...     ...    ...   \n",
      "995    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "996    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "997    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "998    0.0      0.0  0.0   0.0     0.0  0.237555    0.0     0.0    0.0   \n",
      "999    0.0      0.0  0.0   0.0     0.0  0.000000    0.0     0.0    0.0   \n",
      "\n",
      "     abrupt  ...  zambi  zelda  zeldaroguelik  zerog  zip  zombi  zone  \\\n",
      "0       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "1       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "2       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "3       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "4       0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "..      ...  ...    ...    ...            ...    ...  ...    ...   ...   \n",
      "995     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "996     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "997     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "998     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "999     0.0  ...    0.0    0.0            0.0    0.0  0.0    0.0   0.0   \n",
      "\n",
      "         zoom  zue   ·Öö·Öö  \n",
      "0    0.000000  0.0  0.0  \n",
      "1    0.000000  0.0  0.0  \n",
      "2    0.000000  0.0  0.0  \n",
      "3    0.027886  0.0  0.0  \n",
      "4    0.000000  0.0  0.0  \n",
      "..        ...  ...  ...  \n",
      "995  0.000000  0.0  0.0  \n",
      "996  0.000000  0.0  0.0  \n",
      "997  0.000000  0.0  0.0  \n",
      "998  0.000000  0.0  0.0  \n",
      "999  0.000000  0.0  0.0  \n",
      "\n",
      "[1000 rows x 4467 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get weights of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "tfidf_matrix = vect.fit_transform(x_train[:1000])\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98719733 0.45785863 0.8969804  ... 0.86328383 0.97517055 0.88208671]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "y_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]\n",
    "print(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.24      0.33     16673\n",
      "           1       0.89      0.97      0.93    108327\n",
      "\n",
      "    accuracy                           0.87    125000\n",
      "   macro avg       0.72      0.60      0.63    125000\n",
      "weighted avg       0.85      0.87      0.85    125000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.796004679317431"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 10000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "  0%|          | 0/10000 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Johan\\Documents\\GitHub\\NLP-projects\\steam_reviews.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_test_dtm_sample \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39msample(x_train_dtm, \u001b[39m10000\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mKernelExplainer(nb\u001b[39m.\u001b[39mpredict, x_test_dtm_sample)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(x_test_dtm_sample)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Johan/Documents/GitHub/NLP-projects/steam_reviews.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m shap\u001b[39m.\u001b[39msummary_plot(shap_values, x_test_dtm_sample)\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:242\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index:\n\u001b[0;32m    241\u001b[0m     data \u001b[39m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], index_name)\n\u001b[1;32m--> 242\u001b[0m explanations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgc_collect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    244\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:326\u001b[0m, in \u001b[0;36mKernel.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsamples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_samples\n\u001b[0;32m    325\u001b[0m \u001b[39m# reserve space for some of our computations\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallocate()\n\u001b[0;32m    328\u001b[0m \u001b[39m# weight the different subset sizes\u001b[39;00m\n\u001b[0;32m    329\u001b[0m num_subset_sizes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mM \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:525\u001b[0m, in \u001b[0;36mKernel.allocate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    523\u001b[0m         new_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsamples)\n\u001b[0;32m    524\u001b[0m         new_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(indices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsamples)\n\u001b[1;32m--> 525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynth_data \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mcsr_matrix((new_data, new_indices, new_indptr), shape\u001b[39m=\u001b[39;49mshape)\u001b[39m.\u001b[39;49mtolil()\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    527\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynth_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdata, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsamples, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Johan\\.virtualenvs\\NLP-projects-czQ2DZYv\\Lib\\site-packages\\scipy\\sparse\\_csr.py:163\u001b[0m, in \u001b[0;36m_csr_base.tolil\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    161\u001b[0m     end \u001b[39m=\u001b[39m ptr[n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    162\u001b[0m     rows[n] \u001b[39m=\u001b[39m ind[start:end]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m--> 163\u001b[0m     data[n] \u001b[39m=\u001b[39m dat[start:end]\u001b[39m.\u001b[39;49mtolist()\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m lil\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shap\n",
    "#shap.initjs()\n",
    "x_test_dtm_sample = shap.sample(x_train_dtm, 10000)\n",
    "explainer = shap.KernelExplainer(nb.predict, x_test_dtm_sample)\n",
    "shap_values = explainer.shap_values(x_test_dtm_sample)\n",
    "shap.summary_plot(shap_values, x_test_dtm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-projects-czQ2DZYv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
